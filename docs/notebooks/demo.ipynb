{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/cesargallego/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/cesargallego/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting data-refinery\n",
      "\u001b[31m  Could not find a version that satisfies the requirement data-refinery (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for data-refinery\u001b[0m\n",
      "\u001b[33mThe directory '/home/cesargallego/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/cesargallego/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: pyspark in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: py4j==0.10.4 in /usr/local/lib/python2.7/dist-packages (from pyspark)\n"
     ]
    }
   ],
   "source": [
    "!pip install data-refinery \n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'datarefinery'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8ddd7e6e3d35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatarefinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTupleDSL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatarefinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTupleOperations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatarefinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFieldOperations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_column_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'datarefinery'"
     ]
    }
   ],
   "source": [
    "from datarefinery.tuple.TupleDSL import compose\n",
    "from datarefinery.TupleOperations import append, substitution, wrap\n",
    "from datarefinery.FieldOperations import date_parser, explode_date, remove_columns, add_column_prefix, column_category\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_categories = ['cold', 'warm', 'hot']\n",
    "\n",
    "row = {'id': 1, 'fecha': '2017-02-23', 'weather': 'cold'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_date = compose(\n",
    "        date_parser([\"%Y-%m-%d\"]),\n",
    "        explode_date,\n",
    "        remove_columns(\"hour\", \"minute\", \"second\"),\n",
    "        add_column_prefix(\"x\")\n",
    ")\n",
    "\n",
    "operation = Tr(append(['weather'], column_category(weather_categories))).then(append(['fecha'], complete_date)).apply()\n",
    "\n",
    "(inp, out, err) = operation(row)\n",
    "\n",
    "print('Input: ', inp)\n",
    "print('Ouput: ', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = sc.parallelize([row])\n",
    "out_rdd = weather.map(operation).map(lambda x: x[1])\n",
    "\n",
    "print(out_rdd.collect())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
